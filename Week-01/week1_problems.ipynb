{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "34034f63",
      "metadata": {
        "id": "34034f63"
      },
      "source": [
        "# COMP0189: Applied Artificial Intelligence\n",
        "## Week 1 (Data Preprocessing)\n",
        "\n",
        "### After this week you will be able to ...\n",
        "- load various datasets from sklearn\n",
        "- know the importance of data scaling and preprocessing\n",
        "- know the sensitivity between various learning algorithms\n",
        "- split the dataset into train and test set\n",
        "- know what will happen if you apply different preprocessing steps to train and test set\n",
        "- know how to encode categorical features to ordinal and one-hot representations and how these affect model performance\n",
        "- know how to deal with missing data\n",
        "\n",
        "### Acknowledgements\n",
        "- https://github.com/UCLAIS/Machine-Learning-Tutorials\n",
        "- https://www.cs.columbia.edu/~amueller/comsw4995s19/schedule/\n",
        "- https://scikit-learn.org/stable/\n",
        "- https://archive.ics.uci.edu/ml/datasets/adult"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a28f1108",
      "metadata": {
        "id": "a28f1108"
      },
      "source": [
        "## Introduction to Scikit-learn\n",
        "\n",
        "Why do we use sklearn??\n",
        "\n",
        "1. Example Datasets\n",
        "    - sklearn.datasets : Provides example datasets\n",
        "\n",
        "2. Feature Engineering  \n",
        "    - sklearn.preprocessing : Variable functions as to data preprocessing\n",
        "    - sklearn.feature_selection : Help selecting primary components in datasets\n",
        "    - sklearn.feature_extraction : Vectorised feature extraction\n",
        "    - sklearn.decomposition : Algorithms regarding Dimensionality Reduction\n",
        "\n",
        "3. Data split and Parameter Tuning  \n",
        "    - sklearn.model_selection : 'Train Test Split' for cross validation, Parameter tuning with GridSearch\n",
        "\n",
        "4. Evaluation  \n",
        "    - sklearn.metrics : accuracy score, ROC curve, F1 score, etc.\n",
        "\n",
        "5. ML Algorithms\n",
        "    - sklearn.ensemble : Ensemble, etc.\n",
        "    - sklearn.linear_model : Linear Regression, Logistic Regression, etc.\n",
        "    - sklearn.naive_bayes : Gaussian Naive Bayes classification, etc.\n",
        "    - sklearn.neighbors : Nearest Centroid classification, etc.\n",
        "    - sklearn.svm : Support Vector Machine\n",
        "    - sklearn.tree : DecisionTreeClassifier, etc.\n",
        "    - sklearn.cluster : Clustering (Unsupervised Learning)\n",
        "\n",
        "6. Utilities  \n",
        "    - sklearn.pipeline: pipeline of (feature engineering -> ML Algorithms -> Prediction)\n",
        "\n",
        "7. Train and Predict  \n",
        "    - fit()\n",
        "    - predict()\n",
        "\n",
        "8. and more..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bb38f49",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bb38f49",
        "outputId": "4f561fe7-767a-49c9-b2fd-dd921d5bf0d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn==1.1.3\n",
            "  Downloading scikit_learn-1.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.5/30.5 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.3) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.3) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.3) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.3) (3.2.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 0.18.0 requires scikit-learn>=1.2.2, but you have scikit-learn 1.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed scikit-learn-1.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn==1.1.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e0eca34",
      "metadata": {
        "id": "7e0eca34"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_boston\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore', category=FutureWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b5fdd50",
      "metadata": {
        "id": "6b5fdd50"
      },
      "source": [
        "**1. Boston House Price Dataset**\n",
        "\n",
        "Let's first take a look at the Boston House Price dataset. This Dataset is deprecated as of version 1.2, but we will use this for educational purpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8783fee",
      "metadata": {
        "id": "d8783fee"
      },
      "outputs": [],
      "source": [
        "boston = load_boston()\n",
        "print(boston.DESCR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "111eb500",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "111eb500",
        "outputId": "567d6061-6434-406b-e532-54319693d329"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename', 'data_module'])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "boston.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52fffb8b",
      "metadata": {
        "id": "52fffb8b"
      },
      "outputs": [],
      "source": [
        "boston.feature_names, len(boston.feature_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2004f854",
      "metadata": {
        "id": "2004f854"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X, y = boston.data, boston.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eefdb9d4",
      "metadata": {
        "id": "eefdb9d4"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(3, 5, figsize=(20, 10))\n",
        "for i, ax in enumerate(axes.ravel()):\n",
        "    if i > 12:\n",
        "        ax.set_visible(False)\n",
        "        continue\n",
        "    ax.plot(X[:, i], y, 'o', alpha=.5)\n",
        "    ax.set_title(\"{}: {}\".format(i, boston.feature_names[i]))\n",
        "    ax.set_ylabel(\"PRICE\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e58cc370",
      "metadata": {
        "id": "e58cc370"
      },
      "source": [
        "See how our data are spread in different ranges. 3rd feature (CHAS) is even in binary. Most of the algorithms perform poorly on these various input spaces."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e949cac4",
      "metadata": {
        "id": "e949cac4"
      },
      "source": [
        "**2. Wine Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "936d1584",
      "metadata": {
        "id": "936d1584"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_wine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17250624",
      "metadata": {
        "id": "17250624"
      },
      "outputs": [],
      "source": [
        "wine = load_wine()\n",
        "print(wine.DESCR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e30564c",
      "metadata": {
        "id": "9e30564c"
      },
      "outputs": [],
      "source": [
        "wine.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d81aa2b",
      "metadata": {
        "id": "0d81aa2b"
      },
      "outputs": [],
      "source": [
        "wine_X = wine.data\n",
        "wine_labels = wine.target\n",
        "wine_feature_names = wine.feature_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0d26d44",
      "metadata": {
        "id": "d0d26d44"
      },
      "outputs": [],
      "source": [
        "wine_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7d5e1c5",
      "metadata": {
        "id": "c7d5e1c5"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(wine_X, columns=wine_feature_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2dcb7a8",
      "metadata": {
        "id": "c2dcb7a8"
      },
      "outputs": [],
      "source": [
        "def visualise_wine(X, labels=None, column_indices=(0,1), set_labels=False):\n",
        "    \"\"\"\n",
        "    @param: X        --> Data\n",
        "    @param: lables   --> Default is set to None, but if you've got your result of labels from clustering,\n",
        "                         you can input according labels in a list format.\n",
        "    @param: column_indices --> column indices of dataset X to be selected for plotting.\n",
        "                                 two-element tuple if you want 2D graph,\n",
        "                                 three-element tuple if you want 3D graph.\n",
        "    \"\"\"\n",
        "    assert type(column_indices) is tuple\n",
        "\n",
        "    if len(column_indices)==2:  # 2D\n",
        "        first_col, second_col = column_indices[0], column_indices[1]\n",
        "\n",
        "        if set_labels:\n",
        "            plt.xlabel(wine_feature_names[first_col])\n",
        "            plt.ylabel(wine_feature_names[second_col])\n",
        "\n",
        "        plt.scatter(X[:, first_col], X[:, second_col], c=labels)\n",
        "\n",
        "    elif len(column_indices)==3:  # 3D\n",
        "        first_col, second_col, third_col = column_indices[0], column_indices[1], column_indices[2]\n",
        "        fig = plt.figure()\n",
        "        plt.clf()\n",
        "        ax = fig.add_subplot(projection='3d')\n",
        "\n",
        "        plt.cla()\n",
        "\n",
        "        if set_labels:\n",
        "            ax.set_xlabel(wine_feature_names[first_col])\n",
        "            ax.set_ylabel(wine_feature_names[second_col])\n",
        "            ax.set_zlabel(wine_feature_names[third_col])\n",
        "\n",
        "        ax.scatter(X[:, first_col], X[:, second_col], X[:, third_col], c=labels)\n",
        "\n",
        "    else:\n",
        "        raise RuntimeError(\"Your dimension should be either set to \\\"2d\\\" or \\\"3d\\\"\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c36356fb",
      "metadata": {
        "id": "c36356fb"
      },
      "outputs": [],
      "source": [
        "visualise_wine(wine_X, labels=wine_labels, column_indices=(8, 10), set_labels=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "754a3fd1",
      "metadata": {
        "id": "754a3fd1"
      },
      "outputs": [],
      "source": [
        "# try out different col_in_X and get some feeling of how the data is shaped.\n",
        "visualise_wine(wine_X, labels=wine_labels, column_indices=(8, 10, 12), set_labels=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b99a2680",
      "metadata": {
        "id": "b99a2680"
      },
      "source": [
        "We will closely look into details of many functions in scikit-learn (fit, predict, PCA, metrics, etc.) in the following practicals as we learn more in lectures.  \n",
        "For now, it is good to be familiar with datasets and the main takeaways we demonstrate."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cee6973e",
      "metadata": {
        "id": "cee6973e"
      },
      "source": [
        "## Exercise 1: Impact of feature scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e5dede2",
      "metadata": {
        "id": "8e5dede2"
      },
      "source": [
        "Normalization scales each input variable separately to the range 0-1.  \n",
        "Standardization scales each input variable separately by subtracting the mean (centering) and dividing each of them by the standard deviation to shift the distribution to have a mean of zero and a standard deviation of one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18994d2f",
      "metadata": {
        "id": "18994d2f"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c96b1bba",
      "metadata": {
        "id": "c96b1bba"
      },
      "source": [
        "#### Example usage of sklearn.preprocessing.StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ce7c672",
      "metadata": {
        "id": "4ce7c672"
      },
      "outputs": [],
      "source": [
        "# Example\n",
        "unscaled_data = np.asarray([[100, 0.001],\n",
        " [8, 0.05],\n",
        " [50, 0.005],\n",
        " [88, 0.07],\n",
        " [4, 0.1]])\n",
        "# define standard scaler\n",
        "scaler = StandardScaler()\n",
        "# transform data\n",
        "scaled_data = scaler.fit_transform(unscaled_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c79b31b",
      "metadata": {
        "id": "4c79b31b"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(unscaled_data).hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54b7ceb7",
      "metadata": {
        "id": "54b7ceb7"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(scaled_data).hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1244e3e6",
      "metadata": {
        "id": "1244e3e6"
      },
      "outputs": [],
      "source": [
        "del scaled_data, unscaled_data, scaler"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe24e99f",
      "metadata": {
        "id": "fe24e99f"
      },
      "source": [
        "**Questions**  \n",
        "- Try using different scaling methods, such as MinMaxScaler and Normalisation. Do you see the difference in the histogram?\n",
        "- Experiment the effects of different feature scaling methods on various ML algorithms e.g. KNN, SVM, Decision-Tree."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "465fa5bd",
      "metadata": {
        "id": "465fa5bd"
      },
      "source": [
        "### Scaling Vs. Unscaling the Wine Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffdfa4ba",
      "metadata": {
        "id": "ffdfa4ba"
      },
      "outputs": [],
      "source": [
        "RANDOM_STATE = 42\n",
        "# We are using the wind dataset again\n",
        "features, target = load_wine(return_X_y=True)\n",
        "\n",
        "# Make a train/test split using 30% test size\n",
        "# Make a train/test split using 30% test size\n",
        "X_train, X_test, y_train, y_test = train_test_split(None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jF8ckAVlYkk2",
      "metadata": {
        "id": "jF8ckAVlYkk2"
      },
      "outputs": [],
      "source": [
        "# Define scalers and models\n",
        "scalers = {\n",
        "    'StandardScaler': StandardScaler(),\n",
        "    'MinMaxScaler': MinMaxScaler(),\n",
        "    'Normalizer': Normalizer()\n",
        "}\n",
        "\n",
        "models = {\n",
        "    'KNeighborsClassifier': KNeighborsClassifier(),\n",
        "    'SVC': SVC(random_state=RANDOM_STATE),\n",
        "    'DecisionTreeClassifier': DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
        "}\n",
        "\n",
        "# Store results\n",
        "results = {}\n",
        "\n",
        "# Iterate over each scaler\n",
        "for scaler_name, scaler in scalers.items():\n",
        "    scaled_X_train = scaler.None\n",
        "    scaled_X_test = scaler.None\n",
        "\n",
        "    # Iterate over each model\n",
        "    for model_name, model in models.items():\n",
        "        key = f'{scaler_name}_{model_name}'\n",
        "\n",
        "        # Fit and predict with unscaled and scaled data\n",
        "        model.fit(None)\n",
        "        unscaled_y_hat = model.predict(None)\n",
        "        unscaled_acc = accuracy_score(None)\n",
        "\n",
        "        model.fit(None)\n",
        "        scaled_y_hat = model.predict(None)\n",
        "        scaled_acc = accuracy_score(None)\n",
        "\n",
        "        # Store results\n",
        "        results[key] = {\n",
        "            'Unscaled Accuracy': unscaled_acc,\n",
        "            'Scaled Accuracy': scaled_acc\n",
        "        }\n",
        "\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91b715b4",
      "metadata": {
        "id": "91b715b4"
      },
      "source": [
        "## Exercise 2: Impact of different preprocessing strategy in train and test data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5caf828b",
      "metadata": {
        "id": "5caf828b"
      },
      "source": [
        "Do you see the difference in accuracy?  \n",
        "**Question**  \n",
        "Now, notice that I also scaled the test set.   \n",
        "Using the same code, see what happens if you don't scale the test data and predict based on the unscaled data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VURv48wAZ6k9",
      "metadata": {
        "id": "VURv48wAZ6k9"
      },
      "outputs": [],
      "source": [
        "# Store results\n",
        "results = {}\n",
        "\n",
        "# Iterate over each scaler\n",
        "for scaler_name, scaler in scalers.items():\n",
        "    scaled_X_train = scaler.fit_transform(None)\n",
        "    scaled_X_test = scaler.transform(None)\n",
        "\n",
        "    # Iterate over each model\n",
        "    for model_name, model in models.items():\n",
        "        key = f'{scaler_name}_{model_name}'\n",
        "\n",
        "        # Fit with scaled data\n",
        "        model.fit(scaled_X_train, y_train)\n",
        "\n",
        "        # Predict with unscaled test data\n",
        "        unscaled_y_hat = model.None\n",
        "        unscaled_acc = accuracy_score(None)\n",
        "\n",
        "        # Predict with scaled test data\n",
        "        scaled_y_hat = model.predict(None)\n",
        "        scaled_acc = accuracy_score(None)\n",
        "\n",
        "        # Store results\n",
        "        results[key] = {\n",
        "            'Accuracy with Unscaled Test Data': unscaled_acc,\n",
        "            'Accuracy with Scaled Test Data': scaled_acc\n",
        "        }\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df\n",
        "\n",
        "# Display results\n",
        "#for key, value in results.items():\n",
        "#    print(key, value)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "232ba604",
      "metadata": {
        "id": "232ba604"
      },
      "source": [
        "## Now we move on the next session which is about categorial features and data imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18a61ed9",
      "metadata": {
        "id": "18a61ed9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_selection import mutual_info_classif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6977889",
      "metadata": {
        "id": "d6977889"
      },
      "outputs": [],
      "source": [
        "# Open the csv file and skim through it. It does not have column names\n",
        "# so we will allocate names to each column\n",
        "\n",
        "# Naming the Columns\n",
        "names = ['age','workclass','fnlwgt','education','education-num',\n",
        "        'marital-status','occupation','relationship','race','sex',\n",
        "        'capital-gain','capital-loss','hours-per-week','native-country',\n",
        "        'y']\n",
        "\n",
        "# Load dataset with specifying ' ?' as missing values\n",
        "df = pd.read_csv('/adult.data', delimiter=',', names=names, na_values=' ?')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AyRpMIOErkHo",
      "metadata": {
        "id": "AyRpMIOErkHo"
      },
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "An_vzHiIroYf",
      "metadata": {
        "id": "An_vzHiIroYf"
      },
      "outputs": [],
      "source": [
        "# Display the 15th row of the DataFrame - notice NaN\n",
        "row_15 = df.iloc[14]\n",
        "print(row_15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99897885",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99897885",
        "outputId": "a596ea8c-13ae-4a26-9a04-ac47828294ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "32561"
            ]
          },
          "execution_count": 196,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L7jFzTUYwCe8",
      "metadata": {
        "id": "L7jFzTUYwCe8"
      },
      "outputs": [],
      "source": [
        "# for now we will drop the rows with NA values\n",
        "\n",
        "df = df.dropna()\n",
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94f0773d",
      "metadata": {
        "id": "94f0773d"
      },
      "outputs": [],
      "source": [
        "# TASK 1: Get the unique values in the race column\n",
        "df['race'].None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c95cd314",
      "metadata": {
        "id": "c95cd314"
      },
      "outputs": [],
      "source": [
        "# TASK 2: Get the unique values in the 'y' column\n",
        "df['y'].None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "245ce71b",
      "metadata": {
        "id": "245ce71b"
      },
      "outputs": [],
      "source": [
        "# TODO: Get the popluation count by race\n",
        "counts = df['race'].value_counts()\n",
        "labels = counts.index\n",
        "\n",
        "# Plot pie chart\n",
        "plt.pie(counts, startangle=90)\n",
        "plt.legend(labels, loc=2,fontsize=8)\n",
        "plt.title(\"Race\",size=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81c51abd",
      "metadata": {
        "id": "81c51abd"
      },
      "outputs": [],
      "source": [
        "# TASK 3\n",
        "# We see redundant space prefix in the values. Remove them.\n",
        "df['race'] = df['race'].apply(None)\n",
        "df['y'] = df['y'].apply(None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71a747d2",
      "metadata": {
        "id": "71a747d2"
      },
      "outputs": [],
      "source": [
        "df['race'].unique(), df['y'].unique(), df['occupation'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ded9523",
      "metadata": {
        "id": "8ded9523"
      },
      "source": [
        "Hmmm it's not just the race and y column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a575e39b",
      "metadata": {
        "id": "a575e39b"
      },
      "outputs": [],
      "source": [
        "# Let's try to apply this to all the string-valued columns\n",
        "for col_name in df.columns:\n",
        "    if df[col_name].dtype == object:  # Checking for object type (string in pandas)\n",
        "        df[col_name] = df[col_name].apply(lambda x: x.strip() if isinstance(x, str) else x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78e919df",
      "metadata": {
        "id": "78e919df"
      },
      "outputs": [],
      "source": [
        "for col_name in df.columns:\n",
        "    if not 'int' in str(df[col_name].dtype):\n",
        "        print(df[col_name].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b51257e6",
      "metadata": {
        "id": "b51257e6"
      },
      "source": [
        "All done!  \n",
        "Now let's specifically look into the 'race' and 'y' columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ec1b341",
      "metadata": {
        "id": "7ec1b341"
      },
      "outputs": [],
      "source": [
        "df[['race', 'y']].head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "116f91d9",
      "metadata": {
        "id": "116f91d9"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder\n",
        "\n",
        "# TASK 4: Convert features and target to binary numerical values using\n",
        "# Ordinal, One-hot, LabelEncoding as appropriate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r1hzAqc8eW_V",
      "metadata": {
        "id": "r1hzAqc8eW_V"
      },
      "outputs": [],
      "source": [
        "# Assuming df is your DataFrame\n",
        "\n",
        "# Ordinal Encoding for 'education'\n",
        "ordinal_encoder = OrdinalEncoder()\n",
        "df['education_encoded'] = ordinal_encoder.fit_transform(df[[None]])\n",
        "\n",
        "# OneHotEncoding for nominal features without an implied order\n",
        "# Including the previously missed nominal columns\n",
        "nominal_columns = [None]\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "onehot_encoded_columns = onehot_encoder.fit_transform(df[nominal_columns])\n",
        "column_names = onehot_encoder.get_feature_names_out(nominal_columns)\n",
        "df_onehot_encoded = pd.DataFrame(onehot_encoded_columns, columns=column_names)\n",
        "\n",
        "# Integrate these new columns back into the original dataframe\n",
        "df = df.reset_index(drop=True)  # Reset index to align with the new onehot encoded DataFrame\n",
        "df = pd.concat([df, df_onehot_encoded], axis=1)\n",
        "\n",
        "# Optionally, remove the categorical columns if no longer needed\n",
        "df.drop(columns=nominal_columns + ['education'], inplace=True)\n",
        "\n",
        "# Label Encoding for the target variable\n",
        "label_encoder = LabelEncoder()\n",
        "df['y_encoded'] = label_encoder.fit_transform(df['y'])\n",
        "\n",
        "# Remove the original 'y' column if no longer needed\n",
        "df.drop(columns=['y'], inplace=True)\n",
        "\n",
        "# Display the first few rows of the modified DataFrame\n",
        "df.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "897db437",
      "metadata": {
        "id": "897db437"
      },
      "source": [
        "### Dealing with Missing data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25831b63",
      "metadata": {
        "id": "25831b63"
      },
      "source": [
        "#### In processing the data earlier, we did not take account of the missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47bf7cbc",
      "metadata": {
        "id": "47bf7cbc"
      },
      "outputs": [],
      "source": [
        "# Re-Load dataset with specifying ' ?' as missing values\n",
        "df = pd.read_csv('/adult.data', delimiter=',', names=names, na_values=' ?')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6a0c2dd",
      "metadata": {
        "id": "d6a0c2dd",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# TASK 7 Create 3 datasets using different methods for dealing with missing data:\n",
        "# A: Drop missing values, B: KNN imputation, C: Most frequest imputation\n",
        "\n",
        "for col_name in df.columns:\n",
        "    if df[col_name].dtype == object:  # Checking for object type (string in pandas)\n",
        "        df[col_name] = df[col_name].apply(lambda x: x.strip() if isinstance(x, str) else x)\n",
        "\n",
        "# Check for missing values\n",
        "print(df.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_OjRTt683uBW",
      "metadata": {
        "id": "_OjRTt683uBW"
      },
      "outputs": [],
      "source": [
        "# first conduct encoding for features without missing values\n",
        "\n",
        "# Ordinal Encoding for 'education'\n",
        "ordinal_encoder = OrdinalEncoder()\n",
        "df['education_encoded'] = ordinal_encoder.fit_transform(None)\n",
        "\n",
        "# OneHotEncoding for nominal features without missing values and without an implied order\n",
        "nominal_columns_without_missing = [None]\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "onehot_encoded_columns = onehot_encoder.fit_transform(df[None])\n",
        "column_names = onehot_encoder.get_feature_names_out(None)\n",
        "df_onehot_encoded = pd.DataFrame(None)\n",
        "\n",
        "# Integrate these new columns back into the original dataframe\n",
        "df = df.reset_index(drop=True)  # Reset index to align with the new onehot encoded DataFrame\n",
        "df = pd.concat([df, df_onehot_encoded], axis=1)\n",
        "\n",
        "# Optionally, remove the original categorical columns if no longer needed\n",
        "df.drop(columns=nominal_columns_without_missing + ['education'], inplace=True)\n",
        "\n",
        "# Label Encoding for the target variable\n",
        "label_encoder = LabelEncoder()\n",
        "df['y_encoded'] = label_encoder.fit_transform(df['y'])\n",
        "\n",
        "# Remove the original 'y' column if no longer needed\n",
        "df.drop(columns=['y'], inplace=True)\n",
        "\n",
        "# Display the first few rows of the modified DataFrame\n",
        "df.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vsdn-Zkk4krB",
      "metadata": {
        "id": "vsdn-Zkk4krB"
      },
      "outputs": [],
      "source": [
        "print(df.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ulc8JBkg6YF",
      "metadata": {
        "id": "7ulc8JBkg6YF"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "# A: Dataset with dropped missing values\n",
        "df_dropna = df.None\n",
        "\n",
        "print(df_dropna.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B2Crh-TB5MJG",
      "metadata": {
        "id": "B2Crh-TB5MJG"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "df_knn_imputed = df.copy()\n",
        "\n",
        "# Temporarily encode categorical columns with missing values\n",
        "temp_encoder = OrdinalEncoder()\n",
        "columns_with_missing_values = ['workclass', 'occupation', 'native-country']\n",
        "df_temp = df[columns_with_missing_values].copy()\n",
        "df_temp_encoded = temp_encoder.fit_transform(df_temp)\n",
        "\n",
        "# Apply KNN imputer\n",
        "knn_imputer = KNNImputer(n_neighbors=5)\n",
        "imputed_data = knn_imputer.None\n",
        "\n",
        "# Decode the categorical columns back to original categories\n",
        "imputed_data_decoded = temp_encoder.inverse_transform(imputed_data)\n",
        "df_imputed_final = pd.DataFrame(imputed_data_decoded, columns=columns_with_missing_values)\n",
        "\n",
        "# Integrate the imputed columns back into the main DataFrame\n",
        "df_knn_imputed[columns_with_missing_values] = df_imputed_final\n",
        "\n",
        "print(df_knn_imputed.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hxda87fO5PzL",
      "metadata": {
        "id": "hxda87fO5PzL"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Create an imputer object using the most frequent strategy\n",
        "mode_imputer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "# Apply the imputer to the categorical columns with missing values\n",
        "df_mode_imputed = df.copy()\n",
        "df_mode_imputed[columns_with_missing_values] = mode_imputer.fit_transform(None)\n",
        "\n",
        "print(df_mode_imputed.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zIi-nJ2eA8mD",
      "metadata": {
        "id": "zIi-nJ2eA8mD"
      },
      "outputs": [],
      "source": [
        "def apply_onehot_encoding(df, columns):\n",
        "    # Perform One-Hot Encoding\n",
        "    encoded_data = onehot_encoder.fit_transform(df[columns])\n",
        "    column_names = onehot_encoder.get_feature_names_out(columns)\n",
        "    df_encoded = pd.DataFrame(encoded_data, columns=column_names)\n",
        "\n",
        "    # Reset indices to ensure alignment\n",
        "    df_reset = df.reset_index(drop=True)\n",
        "    df_encoded_reset = df_encoded.reset_index(drop=True)\n",
        "\n",
        "    # Drop original columns and concatenate the new One-Hot Encoded columns\n",
        "    return pd.concat([df_reset.drop(columns, axis=1), df_encoded_reset], axis=1)\n",
        "\n",
        "# Columns to be One-Hot Encoded\n",
        "columns_to_encode = [None]\n",
        "# Apply One-Hot Encoding\n",
        "df_dropna_encoded = apply_onehot_encoding(None)\n",
        "\n",
        "# Check for missing values after encoding\n",
        "print(df_dropna_encoded.isnull().sum().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h9R_FZ5A6zOv",
      "metadata": {
        "id": "h9R_FZ5A6zOv"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# now apply one-hot encoding for the feautres which were imputed\n",
        "\n",
        "# Initialize One-Hot Encoder\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "# Function to apply One-Hot Encoding to a DataFrame\n",
        "def apply_onehot_encoding(df, columns):\n",
        "    encoded_data = onehot_encoder.fit_transform(df[columns])\n",
        "    column_names = onehot_encoder.get_feature_names_out(columns)\n",
        "    df_encoded = pd.DataFrame(encoded_data, columns=column_names)\n",
        "\n",
        "    # Drop original columns and concatenate the new One-Hot Encoded columns\n",
        "    return pd.concat([df.drop(columns, axis=1), df_encoded], axis=1)\n",
        "\n",
        "# Apply One-Hot Encoding to each DataFrame\n",
        "\n",
        "df_knn_imputed_encoded = apply_onehot_encoding(None)\n",
        "df_mode_imputed_encoded = apply_onehot_encoding(None)\n",
        "\n",
        "\n",
        "print(df_knn_imputed_encoded.isnull().sum().sum())\n",
        "print(df_mode_imputed_encoded.isnull().sum().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72a28f3f",
      "metadata": {
        "id": "72a28f3f"
      },
      "source": [
        "### Now, train an SVM or KNN Classifier and check the metrics by using the function below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24e991bc",
      "metadata": {
        "id": "24e991bc"
      },
      "outputs": [],
      "source": [
        "# TASK 8: Train an SVM Classifier on the differnt dataset to compare imputation method accuracy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec6683e7",
      "metadata": {
        "id": "ec6683e7"
      },
      "outputs": [],
      "source": [
        "# For dataset A\n",
        "X_dropna = df_dropna_encoded.drop('y_encoded', axis=1)\n",
        "y_dropna = df_dropna_encoded['y_encoded']\n",
        "\n",
        "# For dataset B\n",
        "X_knn = df_knn_imputed_encoded.drop('y_encoded', axis=1)\n",
        "y_knn = df_knn_imputed_encoded['y_encoded']\n",
        "\n",
        "# For dataset C\n",
        "X_mode = df_mode_imputed_encoded.drop('y_encoded', axis=1)\n",
        "y_mode = df_mode_imputed_encoded['y_encoded']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fK63qLXb7x5b",
      "metadata": {
        "id": "fK63qLXb7x5b"
      },
      "outputs": [],
      "source": [
        "# Function to train and evaluate SVM\n",
        "def train_evaluate_svm(X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    clf = SVC()\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00aabc2a",
      "metadata": {
        "id": "00aabc2a"
      },
      "outputs": [],
      "source": [
        "# Train and evaluate on each dataset\n",
        "accuracy_dropna = train_evaluate_svm(None)\n",
        "accuracy_knn = train_evaluate_svm(None)\n",
        "accuracy_mode = train_evaluate_svm(None)\n",
        "\n",
        "# Print the accuracies\n",
        "print(f\"Accuracy with dropped missing values: {accuracy_dropna}\")\n",
        "print(f\"Accuracy with KNN imputation: {accuracy_knn}\")\n",
        "print(f\"Accuracy with mode imputation: {accuracy_mode}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hxg2qYlRitbJ",
      "metadata": {
        "id": "hxg2qYlRitbJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
